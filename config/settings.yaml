# Paper Pilot — default configuration
# See DEV_SPEC §5.5 for full config reference.

llm:
  provider: openai            # openai | azure | local
  model: gpt-4o-mini
  temperature: 0.3
  api_key: ${OPENAI_API_KEY}  # resolved from env at runtime

# RAG MCP Server：独立文件夹时需指定 cwd 和 env（与 Cursor MCP 配置一致）
# rag_default_collection：若 RAG 数据在指定 collection，在此填写；不填则查询时不限定
# 若 stdio 仍卡住：可改用 streamable-http，先单独启动 RAG 服务再连（见下方注释示例）
# mcp:
#   rag_default_collection: null
#   connections:
#     rag_server:
#       transport: stdio
#       command: C:\code\python\MODULAR-RAG-MCP-SERVER\.venv\Scripts\python.exe
#       args: ["-m", "src.mcp_server.server"]
#       cwd: C:\code\python\MODULAR-RAG-MCP-SERVER
#       mcp_stderr: devnull
#       env:
#         OPENAI_API_KEY: ${OPENAI_API_KEY}
#     # 若 stdio 卡住，可改为 HTTP：先在 RAG 项目里单独启动服务，再填 url，例如：
#     # rag_server:
#     #   transport: streamable-http
#     #   url: http://127.0.0.1:8000/mcp

mcp:
  rag_default_collection: essays
  connections:
    rag_server:
      transport: streamable-http
      url: http://127.0.0.1:8000/mcp

agent:
  max_retries: 2
  max_react_steps: 5
  router_model_path: null     # local LoRA path (optional)
  critic_model_path: null     # local DPO  path (optional)

memory:
  memory_file: data/long_term_memory.jsonl

tracing:
  trace_dir: data/traces
  trace_file: trace.jsonl

vllm:
  enabled: false
  base_url: http://localhost:8000/v1
  api_key: token-placeholder
  router_model: router-lora
  critic_model: critic-lora
